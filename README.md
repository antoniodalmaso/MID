# Minimum Impurity Discretizer (MID)
This repository contains the code related to the development and testing of the Minimum Impurity Discretizer (MID) algorithm. MID is a new discretization technique developed to enhance the performance of Optimal Decision Tree (ODT) learners when applied to continuous datasets. This technique is designed to be used with [DL8.5](https://ojs.aaai.org/index.php/AAAI/article/view/5711).

The Python implementation of the discretizer can be found in `discretizers/MinimumImpurityDiscretizer.py`.

## How to Run the Experiments
All experiments were conducted on the CECI cluster [Lemaitre4](https://www.ceci-hpc.be/clusters.html#lemaitre4), which uses the Slurm job scheduler. To reproduce the results, execute the bash scripts in the main folder using the command:

```
sbatch script_name.sh
```

Run the scripts in the following order:
1. **_submit_mid.sh_**: Trains DL8.5 and CART on a single dataset discretized using MID, using all numbers of features between 1 and 45. Modify line 13 of the script to specify the dataset, the maximum depth of the trees, and the minimum support of the leaves (in this order).
2. **_submit_mdlp.sh_**: Trains DL8.5 on a single dataset discretized using MDLP. Modify line 13 of the script to specify the dataset.
3. **_submit_equal_freq.sh_**: Trains DL8.5 on a single dataset discretized using 8-bin equal frequency. Modify line 13 of the script to specify the dataset.
4. **_submit_cart_only.sh_**: Trains and tests CART on all original continuous datasets.
5. **_submit_aggregate.sh_**.
6. **_submit_plot.sh_**.

## Credits
The implementation of the [MDLP discretization technique](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Fayyad+U.M.%2C+Irani+K.B.%3A+Multi-interval+discretization+of+continuous-valued+at-+tributes+for+classification+learning.+Proceedings+of+the+Thirteenth+International+Joint+Conference+on+Artificial+Intelligence%2C+pp.+1022%E2%80%931029.+Morgan+Kaufmann+Publishers%2C+San+Francisco+%281993%29.&btnG=) used for comparison with MID was developed by Victor Ruiz (vmr11@pitt.edu). The original repository can be found [here](https://github.com/navicto/Discretization-MDLPC.git).

# MID Pseudocode
The goal of MID is to produce an ordered list of binary features from continuous input data. Binary datasets of different dimensionalities can then be generated by selecting shorter or longer prefixes of this list. This algorithm uses a top-down strategy, where the range of every single feature is iteratively split in two intervals using an impurity metric. For now, entropy and the Gini index are available options. At each iteration, the chosen metric is used to evaluate the quality of the possible splits, and thus to choose the best one. Only boundary points are considered as candidate thresholds for the disctretization, namely values that separate instances of the dataset belonging to different classes. It is possible to demonstrate that these are the only values that can minimize the considered impurity metrics. Doing this reduces the search space of the candidate thresholds and speeds up execution.

Assuming entropy is used as the impurity metric, MID applies the following preliminary operations to each continuous feature $A$:
1. The observations of $A$ are reordered along with their class labels.
2. The boundary points are retrieved to be used as candidates. We use $T_c$ to refer to the set containing all of them.
3. The class entropy related to $A$ is computed for every $t$ in $T_c$.
4. The candidate $t^∗$ with the smallest entropy value is stored separately, together with its entropy gain. We define it as the difference between the entropy of $A$ before and after applying $t^∗$.
5. $t^∗$ is removed from $T_c$.
6. Steps 3 to 5 are repeated until $T_c$ is empty.

For example, consider the scenario in the fpllowing picture, where $T_c =$ { $t_1$, $t_2$ } and $t$ (which was previously selected) divides the observations of $A$ in two subsets $S_1$ and $S_2$. If applied, $t_1$ would split $S_1$ in $S_{1,1}$ and $S_{1,2}$. In the same way, $t_2$ would divide $S_2$ in $S_{2,1}$ and $S_{2,2}$. Let $S$ be the full set of samples.

<div align="center">
  <img src="https://github.com/user-attachments/assets/6e374f30-0486-4433-b75a-6f8c0ce4ad3e" width="500"/>
</div>

The entropy scores associated to the candidates are

$$E(t_1) = \frac{|S_{1,1}|}{|S|} H(C|S_{1,1}) + \frac{|S_{1,2}|}{|S|} H(C|S_{1,2}) + \frac{|S_2|}{|S|} H(C|S_2) ,$$

$$E(t_2) = \frac{|S_1|}{|S|} H(C|S_1) + \frac{|S_{2,1}|}{|S|} H(C|S_{2,1}) + \frac{|S_{2,2}|}{|S|} H(C|S_{2,2}) .$$

Performing these operations for all the boundary points of a continuous feature establishes a ranking among them. Each element of the ranking is considered to be the best threshold to discretize the continuous feature, given that all the elements occupying a higher position have already been applied.

MID avoids redundant computation of class entropies by using a cache to store the terms $H(C|S_k)$ for every subset $S_k$. Let $E_i$ denote the total entropy of the class labels at the end of the $i$-th iteration, after $i$ thresholds have already been removed from $T_c$. Let $t$ be a candidate that would split a subset $S_k$ into $S_{k,1}$ and $S_{k,2}$. Then, the entropy score associated with $t$ at iteration $i+1$ is

$$E(t) = E_i - \frac{|S_k|}{|S|} H(C|S_k) + \frac{|S_{k,1}|}{|S|} H(C|S_{k,1}) + \frac{|S_{k,2}|}{|S|} H(C|S_{k,2}) .$$

By storing the terms $H(C|S_k)$, $H(C|S_{k,1})$ and $H(C|S_{k,2})$ at each iteration, it becomes easier to compute the class entropy for each candidate by leveraging previous computations.

The pseudocode for training a minimum impurity discretizer when applied to a full dataset is reported below. Lines 1 to 6 implement the steps described above: for each feature, boundary points are computed (line 4) and sorted using an impurity metric (line 5). A list containing the ranked boundary points and their associated gains is created and saved. It is then possible to produce a global ranking of binary features by merging together these lists (line 7).

## Considerations about the implementation
When two consecutive observations have different class labels, there are infinitely many possible thresholds that can be chosen to separate them. The function that computes the boundary points returns the average between the two values (line 14). Even if two consecutive observations share the same class label, they can still produce a boundary point. This occurs when at least one other sample, identical to one of the two observations, belongs to a different class (lines 19-22).

Regarding the procedure `sort_boundary_points`, it contains an abuse of notation. Variables $LI$ and $RI$ (lines 35-36) are treated as arrays of indices, but this is problematic as the lists $y_{sorted}$ and $b\_points$ (lines 38, 40, 53, 53) are not guaranteed to have the same number of elements. Consequently, using the same indices for both lists is not meaningful. Let $S$ represent the range of values that is being split. $LI$ and $RI$ should be understood as sub-intervals of $S$, with $LI$ representing the interval preceding the candidate split point and $RI$ representing the interval following it. Therefore, the notation $list[I]$ (where $I$ refers to the same type of interval as $RI$ or $LI$) should be interpreted as "the subset of $list$ corresponding to the elements in $I$". The function `compute_entropy` (lines 27, 38, 40) can be replaced with one that computes the Gini index.

Eventually, `get_best_thresholds` produces a global ranking of binary features by merging together the lists returned by `sort_boundary_points`. It also retrieves the top $N$ thresholds in such a ranking. The procedure compares the head of the threshold lists for each feature by entropy gain, and retrieves the best one while removing it (lines 58-67). This can be executed until $N$ binary features are retrieved, or until no more thresholds are available (lines 68, 69). In the resulting ranking, the relative orders of the original features are maintained, as shown in the following picture.

<div align="center">
  <img src="https://github.com/user-attachments/assets/39da62e6-d07b-4833-a4bc-e57c7dd275ba" width="500"/>
</div>

Let $N_1$, $N_2$ be two positive integers, and let $X_1$, $X_2$ be the sets of thresholds obtained by calling `get_best_thresholds` with parameters $N_1$ and $N_2$, respectively. By construction, if $N_1 < N_2$, then $X_1 \subseteq X_2$ making the new set of binary features a superset set of the previous one. MID makes it straightforward to represent the observations in a dataset using a small number of binary features. Moreover, additional features can easily be added if the user has more availability in terms of time and resources. This constitutes an advantage with respect to MDLP, whose output has fixed dimensionality. Once the training process is carried out, the actual discretization can be performed by grouping the elements of the $thresholds$ array by feature through the $idx$ value of each record (line 70), and by applying them on the original attributes of the dataset.
Finally, it is important to notice that producing more than one set of binary features starting from the same continuous dataset only requires to train MID a single time.

<div align="center">
  <img src="https://github.com/user-attachments/assets/8afc36f8-bf98-4bef-94ec-41ae6323c375"/>
</div>

# Experiments setup and Results
To evaluate the performance of MID, we conducted a series of experiments aimed at answering the following questions:
1. How does the DL8.5 algorithm perform on continuous data when it is paired with the minimum impurity discretizer, and how good is this approach compared with other decision tree learners?
2. How does MID behave compared with other discretization techniques?

All experiments were conducted using 14 datasets from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/) on a cluster with 5120 AMD Epyc Genoa cores. Each node had 766GB of available RAM, running on Rocky Linux version 8.6. Our experiments compare five classification pipelines: DL8.5 preceded by MID, DL8.5 preceded by the MDLP discretizer, CART paired with MID, CART without any discretization strategy, and a model that applies an 8 bin equal-frequency discretization on the data before feeding it to DL8.5. The classifiers paired with MID have been tested multiple times using different numbers of binary input features.
All experiments were conducted using 10-fold cross-validation with a maximum depth constraint between 3 and 6. To mitigate overfitting, each leaf of the tree was required to have at least five examples. A time limit of 5 minutes was set for the training of DL8.5. Both entropy and Gini index were used as impurity metrics for MID. 

In the experiments, we limit the maximum number of binary features produced using MID to an arbitrary number of 45. This limitation is due to the fact that the number of trees considered by an optimal decision tree algorithm increases exponentially with the number of binary features. A lower bound for this number is given by:

$$N_{trees} = \prod_{i = 0}^{k-1} (N - i)^{2^i} ,$$

where $N$ is the number of binary features and $k$ the maximum depth. For $k=6$ and $N=45$, this value exceeds $3 \cdot 10^{101}$. Despite the branch-and-bound and caching strategies implemented in DL8.5, an excessively large search space can cause the algorithm to stop early before reaching the optimal solution. Limiting the number of features to $45$ ensures that most experiments complete the training process within 5 minutes. Additionally, all 14 UCI datasets can be discretized into a number of features within this range.

To answer our questions, we extract binary features using MID from a continuous dataset and compare DL8.5 results on the obtained binary dataset to CART results on both continuous and binary datasets. We use two different heuristics, entropy and the Gini index, to extract binary features. The plots depicting the results of all the experiments are available in the folder "plots" of the repository. Moreover, by clicking on [this link](https://github.com/user-attachments/files/16848117/tables.pdf), you can access all the test set results.

Table 3.2 contains the test set accuracies achieved by the five classification pipelines introduced at the beginning of this section, for all the considered datasets and values of maximum depth. For the classifiers paired with the minimum impurity discretizer, the performance obtained after being trained on 45 binary features has been considered. DL8.5 attained the highest test accuracy in 20 of the 56 experiments when it was paired with MID, using either the entropy or the Gini index to evaluate the goodness of the splits. This is the pipeline that, tied with CART trained on continuous features, achieves the best result in the largest number of experiments.
Since it is possible that a plateau was reached at 45 features, in Table 3.3 we repeat the comparison considering the best possible accuracies obtained by DL8.5 and CART when preceded by MID, with a number of features ranging from 1 to 45. This comparison shows a raise in the number of instances where DL8.5 achieves the best test accuracy to 38 cases.



